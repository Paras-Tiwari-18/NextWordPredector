{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Paras-Tiwari-18/NextWordPredector/blob/main/NextWordPredictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('webtext')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "R_dY9lTkk5wc",
        "outputId": "7da6a53c-d59c-47c1-b7d4-f77baed1a743",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "R_dY9lTkk5wc",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown, reuters, webtext\n",
        "all_words = []\n",
        "for fileid in webtext.fileids():\n",
        "    all_words.extend(webtext.words(fileid))\n",
        "\n",
        "print(\"Total words before cleaning:\", len(all_words))"
      ],
      "metadata": {
        "id": "BzqeIF7tlAuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1687f7d0-ecb0-4faa-a993-493ebb540bad"
      },
      "id": "BzqeIF7tlAuf",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words before cleaning: 396733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "unwanted_words = set([\n",
        "    \"19teens\", \"fuck\", \"shit\", \"bitch\", \"www\", \"com\", \"http\", \"te\", \"o\", \"k\", \"e\", \"n\", \"start\", \"shadowbots\"\n",
        "])\n",
        "cleaned_words = []\n",
        "for w in all_words:\n",
        "    w = w.lower().strip()\n",
        "    if w in unwanted_words:\n",
        "        continue\n",
        "    if re.match(r\"^[^a-z]+$\", w):\n",
        "        continue\n",
        "    if len(w) == 1:\n",
        "        continue\n",
        "    cleaned_words.append(w)\n",
        "\n",
        "print(\"Total words after cleaning:\", len(cleaned_words))"
      ],
      "metadata": {
        "id": "l_eMAwoy57ll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7902a4ce-ac9a-40f1-eb0d-b303ee3b190d"
      },
      "id": "l_eMAwoy57ll",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words after cleaning: 282146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_tokens = 40000\n",
        "token_subset = cleaned_words[:max_tokens]\n",
        "print(f\"Using {len(token_subset)} tokens for training\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WXUl843rK6U",
        "outputId": "63ea98fe-4e50-4838-9e5e-ff99355be0b5"
      },
      "id": "9WXUl843rK6U",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 40000 tokens for training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "text = \" \".join(token_subset)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(\"Total unique words:\", total_words)\n",
        "\n",
        "token_list = tokenizer.texts_to_sequences([text])[0]\n"
      ],
      "metadata": {
        "id": "1stEyeMVlI45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac7a0ca9-9fb6-4710-8cec-02d1ecbb4ca3"
      },
      "id": "1stEyeMVlI45",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique words: 3923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = []\n",
        "max_len = 15\n",
        "\n",
        "for i in range(2, len(token_list) + 1):\n",
        "    start = max(0, i - max_len)\n",
        "    n_gram_seq = token_list[start:i]\n",
        "    input_sequences.append(n_gram_seq)\n",
        "\n",
        "print(\"Number of sequences:\", len(input_sequences))\n",
        "\n",
        "max_sequence_len = max(len(seq) for seq in input_sequences)\n",
        "print(\"Max sequence length:\", max_sequence_len)\n",
        "\n"
      ],
      "metadata": {
        "id": "YQzDK49zlosO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a2e47b-c0a5-4ae4-d6fe-fb3757531e45"
      },
      "id": "YQzDK49zlosO",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sequences: 40047\n",
            "Max sequence length: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n"
      ],
      "metadata": {
        "id": "b4rILmowlqmW"
      },
      "id": "b4rILmowlqmW",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training samples:\", X_train.shape)\n",
        "print(\"Validation samples:\", X_val.shape)\n"
      ],
      "metadata": {
        "id": "OAOJAZzkpWUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f39dd74-1247-4242-80e0-fb998b816739"
      },
      "id": "OAOJAZzkpWUR",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: (32037, 14)\n",
            "Validation samples: (8010, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=total_words, output_dim=128, input_length=max_sequence_len - 1),\n",
        "\n",
        "    Bidirectional(LSTM(256, return_sequences=True)),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Bidirectional(LSTM(192, return_sequences=True)),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Bidirectional(LSTM(128)),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "model.build(input_shape=(None, max_sequence_len - 1))\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YH4zu0CLnSuI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "c9a290fd-8e40-4372-a7e8-bcecca4d8f3c"
      },
      "id": "YH4zu0CLnSuI",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m502,144\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │       \u001b[38;5;34m788,480\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m384\u001b[0m)        │     \u001b[38;5;34m1,082,880\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m384\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m525,312\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3923\u001b[0m)           │     \u001b[38;5;34m1,008,211\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">502,144</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">788,480</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,082,880</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3923</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,008,211</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,972,819\u001b[0m (15.16 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,972,819</span> (15.16 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,972,819\u001b[0m (15.16 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,972,819</span> (15.16 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'nextword_best_model.keras',\n",
        "    monitor='accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=350,\n",
        "    batch_size=128,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[checkpoint]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1EdbBMeskPE",
        "outputId": "6cb5ebb3-62d5-442b-88fc-63e7c0608cf8"
      },
      "id": "P1EdbBMeskPE",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0224 - loss: 7.0463\n",
            "Epoch 1: accuracy improved from -inf to 0.02485, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 27ms/step - accuracy: 0.0225 - loss: 7.0442 - val_accuracy: 0.0266 - val_loss: 6.6686\n",
            "Epoch 2/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0266 - loss: 6.5641\n",
            "Epoch 2: accuracy improved from 0.02485 to 0.02978, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.0266 - loss: 6.5641 - val_accuracy: 0.0363 - val_loss: 6.6329\n",
            "Epoch 3/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0355 - loss: 6.4508\n",
            "Epoch 3: accuracy improved from 0.02978 to 0.03699, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.0355 - loss: 6.4508 - val_accuracy: 0.0436 - val_loss: 6.6343\n",
            "Epoch 4/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.0424 - loss: 6.3929\n",
            "Epoch 4: accuracy improved from 0.03699 to 0.04351, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.0424 - loss: 6.3929 - val_accuracy: 0.0482 - val_loss: 6.6276\n",
            "Epoch 5/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0475 - loss: 6.3285\n",
            "Epoch 5: accuracy improved from 0.04351 to 0.04810, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.0475 - loss: 6.3285 - val_accuracy: 0.0484 - val_loss: 6.6391\n",
            "Epoch 6/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0553 - loss: 6.2139\n",
            "Epoch 6: accuracy improved from 0.04810 to 0.05484, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.0553 - loss: 6.2142 - val_accuracy: 0.0588 - val_loss: 6.6440\n",
            "Epoch 7/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0555 - loss: 6.1669\n",
            "Epoch 7: accuracy improved from 0.05484 to 0.05787, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.0555 - loss: 6.1669 - val_accuracy: 0.0593 - val_loss: 6.6008\n",
            "Epoch 8/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0645 - loss: 6.0325\n",
            "Epoch 8: accuracy improved from 0.05787 to 0.06315, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.0645 - loss: 6.0326 - val_accuracy: 0.0615 - val_loss: 6.6208\n",
            "Epoch 9/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0704 - loss: 5.9545\n",
            "Epoch 9: accuracy improved from 0.06315 to 0.06717, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.0703 - loss: 5.9546 - val_accuracy: 0.0613 - val_loss: 6.6526\n",
            "Epoch 10/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0728 - loss: 5.8520\n",
            "Epoch 10: accuracy improved from 0.06717 to 0.07276, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.0728 - loss: 5.8522 - val_accuracy: 0.0672 - val_loss: 6.6766\n",
            "Epoch 11/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0749 - loss: 5.7603\n",
            "Epoch 11: accuracy improved from 0.07276 to 0.07682, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.0749 - loss: 5.7604 - val_accuracy: 0.0668 - val_loss: 6.7630\n",
            "Epoch 12/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0772 - loss: 5.6744\n",
            "Epoch 12: accuracy improved from 0.07682 to 0.07922, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.0773 - loss: 5.6748 - val_accuracy: 0.0760 - val_loss: 6.7640\n",
            "Epoch 13/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0833 - loss: 5.6259\n",
            "Epoch 13: accuracy improved from 0.07922 to 0.08403, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.0833 - loss: 5.6260 - val_accuracy: 0.0778 - val_loss: 6.8330\n",
            "Epoch 14/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0876 - loss: 5.5367\n",
            "Epoch 14: accuracy improved from 0.08403 to 0.08887, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.0876 - loss: 5.5368 - val_accuracy: 0.0800 - val_loss: 6.9912\n",
            "Epoch 15/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.0935 - loss: 5.4412\n",
            "Epoch 15: accuracy improved from 0.08887 to 0.09342, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.0935 - loss: 5.4414 - val_accuracy: 0.0838 - val_loss: 7.0139\n",
            "Epoch 16/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0985 - loss: 5.3851\n",
            "Epoch 16: accuracy improved from 0.09342 to 0.09829, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.0985 - loss: 5.3852 - val_accuracy: 0.0800 - val_loss: 7.1410\n",
            "Epoch 17/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.1014 - loss: 5.3049\n",
            "Epoch 17: accuracy improved from 0.09829 to 0.10091, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.1014 - loss: 5.3051 - val_accuracy: 0.0833 - val_loss: 7.2135\n",
            "Epoch 18/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.1061 - loss: 5.2493\n",
            "Epoch 18: accuracy improved from 0.10091 to 0.10472, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.1061 - loss: 5.2497 - val_accuracy: 0.0801 - val_loss: 7.4024\n",
            "Epoch 19/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.1079 - loss: 5.1929\n",
            "Epoch 19: accuracy improved from 0.10472 to 0.10797, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.1079 - loss: 5.1931 - val_accuracy: 0.0823 - val_loss: 7.4697\n",
            "Epoch 20/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.1111 - loss: 5.1207\n",
            "Epoch 20: accuracy improved from 0.10797 to 0.11081, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.1111 - loss: 5.1210 - val_accuracy: 0.0778 - val_loss: 7.5972\n",
            "Epoch 21/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.1131 - loss: 5.0564\n",
            "Epoch 21: accuracy improved from 0.11081 to 0.11396, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.1131 - loss: 5.0569 - val_accuracy: 0.0824 - val_loss: 7.8073\n",
            "Epoch 22/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.1169 - loss: 4.9980\n",
            "Epoch 22: accuracy improved from 0.11396 to 0.11771, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.1169 - loss: 4.9983 - val_accuracy: 0.0787 - val_loss: 7.8713\n",
            "Epoch 23/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.1273 - loss: 4.9356\n",
            "Epoch 23: accuracy improved from 0.11771 to 0.12345, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.1273 - loss: 4.9358 - val_accuracy: 0.0778 - val_loss: 7.9416\n",
            "Epoch 24/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.1285 - loss: 4.8802\n",
            "Epoch 24: accuracy improved from 0.12345 to 0.12501, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.1284 - loss: 4.8805 - val_accuracy: 0.0790 - val_loss: 8.2230\n",
            "Epoch 25/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.1318 - loss: 4.8118\n",
            "Epoch 25: accuracy improved from 0.12501 to 0.13091, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.1318 - loss: 4.8120 - val_accuracy: 0.0752 - val_loss: 8.1811\n",
            "Epoch 26/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.1368 - loss: 4.7481\n",
            "Epoch 26: accuracy improved from 0.13091 to 0.13463, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.1368 - loss: 4.7483 - val_accuracy: 0.0775 - val_loss: 8.3814\n",
            "Epoch 27/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.1373 - loss: 4.7028\n",
            "Epoch 27: accuracy improved from 0.13463 to 0.13731, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.1373 - loss: 4.7034 - val_accuracy: 0.0742 - val_loss: 8.4037\n",
            "Epoch 28/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.1450 - loss: 4.6747\n",
            "Epoch 28: accuracy improved from 0.13731 to 0.14240, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.1450 - loss: 4.6750 - val_accuracy: 0.0698 - val_loss: 8.5465\n",
            "Epoch 29/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.1484 - loss: 4.6074\n",
            "Epoch 29: accuracy improved from 0.14240 to 0.14642, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.1484 - loss: 4.6075 - val_accuracy: 0.0710 - val_loss: 8.8651\n",
            "Epoch 30/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.1508 - loss: 4.5483\n",
            "Epoch 30: accuracy improved from 0.14642 to 0.14945, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.1508 - loss: 4.5485 - val_accuracy: 0.0714 - val_loss: 8.9170\n",
            "Epoch 31/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.1593 - loss: 4.4868\n",
            "Epoch 31: accuracy improved from 0.14945 to 0.15660, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.1592 - loss: 4.4872 - val_accuracy: 0.0698 - val_loss: 9.2000\n",
            "Epoch 32/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.1654 - loss: 4.4239\n",
            "Epoch 32: accuracy improved from 0.15660 to 0.15872, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.1654 - loss: 4.4241 - val_accuracy: 0.0732 - val_loss: 9.2478\n",
            "Epoch 33/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.1661 - loss: 4.3721\n",
            "Epoch 33: accuracy improved from 0.15872 to 0.16306, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.1661 - loss: 4.3725 - val_accuracy: 0.0722 - val_loss: 9.5474\n",
            "Epoch 34/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.1758 - loss: 4.3090\n",
            "Epoch 34: accuracy improved from 0.16306 to 0.16930, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.1757 - loss: 4.3095 - val_accuracy: 0.0688 - val_loss: 9.5871\n",
            "Epoch 35/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.1730 - loss: 4.2947\n",
            "Epoch 35: accuracy did not improve from 0.16930\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.1730 - loss: 4.2950 - val_accuracy: 0.0699 - val_loss: 9.8573\n",
            "Epoch 36/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.1837 - loss: 4.1956\n",
            "Epoch 36: accuracy improved from 0.16930 to 0.17492, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.1836 - loss: 4.1963 - val_accuracy: 0.0665 - val_loss: 9.7313\n",
            "Epoch 37/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.1789 - loss: 4.1987\n",
            "Epoch 37: accuracy improved from 0.17492 to 0.17870, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.1789 - loss: 4.1988 - val_accuracy: 0.0672 - val_loss: 10.0957\n",
            "Epoch 38/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.1894 - loss: 4.1397\n",
            "Epoch 38: accuracy improved from 0.17870 to 0.18188, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.1893 - loss: 4.1403 - val_accuracy: 0.0688 - val_loss: 10.2539\n",
            "Epoch 39/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.1924 - loss: 4.0774\n",
            "Epoch 39: accuracy improved from 0.18188 to 0.18760, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.1924 - loss: 4.0780 - val_accuracy: 0.0680 - val_loss: 10.3248\n",
            "Epoch 40/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2008 - loss: 4.0506\n",
            "Epoch 40: accuracy improved from 0.18760 to 0.19462, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.2008 - loss: 4.0508 - val_accuracy: 0.0679 - val_loss: 10.6361\n",
            "Epoch 41/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2062 - loss: 4.0002\n",
            "Epoch 41: accuracy improved from 0.19462 to 0.19943, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.2062 - loss: 4.0004 - val_accuracy: 0.0662 - val_loss: 10.7979\n",
            "Epoch 42/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2070 - loss: 3.9271\n",
            "Epoch 42: accuracy improved from 0.19943 to 0.20164, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.2069 - loss: 3.9278 - val_accuracy: 0.0658 - val_loss: 11.0503\n",
            "Epoch 43/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2084 - loss: 3.9112\n",
            "Epoch 43: accuracy improved from 0.20164 to 0.20533, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.2083 - loss: 3.9115 - val_accuracy: 0.0682 - val_loss: 11.2829\n",
            "Epoch 44/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2149 - loss: 3.8499\n",
            "Epoch 44: accuracy improved from 0.20533 to 0.21091, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.2149 - loss: 3.8501 - val_accuracy: 0.0637 - val_loss: 11.1702\n",
            "Epoch 45/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.2173 - loss: 3.8324\n",
            "Epoch 45: accuracy improved from 0.21091 to 0.21584, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.2173 - loss: 3.8326 - val_accuracy: 0.0663 - val_loss: 11.7541\n",
            "Epoch 46/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2228 - loss: 3.7738\n",
            "Epoch 46: accuracy improved from 0.21584 to 0.21703, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.2227 - loss: 3.7745 - val_accuracy: 0.0658 - val_loss: 11.7762\n",
            "Epoch 47/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2268 - loss: 3.7599\n",
            "Epoch 47: accuracy improved from 0.21703 to 0.22162, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.2268 - loss: 3.7603 - val_accuracy: 0.0639 - val_loss: 11.8726\n",
            "Epoch 48/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2359 - loss: 3.7053\n",
            "Epoch 48: accuracy improved from 0.22162 to 0.22827, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.2359 - loss: 3.7056 - val_accuracy: 0.0644 - val_loss: 11.7860\n",
            "Epoch 49/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2373 - loss: 3.6421\n",
            "Epoch 49: accuracy improved from 0.22827 to 0.23267, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.2373 - loss: 3.6423 - val_accuracy: 0.0683 - val_loss: 12.3028\n",
            "Epoch 50/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2401 - loss: 3.6165\n",
            "Epoch 50: accuracy improved from 0.23267 to 0.23395, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.2400 - loss: 3.6170 - val_accuracy: 0.0672 - val_loss: 12.6604\n",
            "Epoch 51/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2428 - loss: 3.6039\n",
            "Epoch 51: accuracy improved from 0.23395 to 0.24053, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.2428 - loss: 3.6040 - val_accuracy: 0.0647 - val_loss: 12.7576\n",
            "Epoch 52/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2487 - loss: 3.5574\n",
            "Epoch 52: accuracy improved from 0.24053 to 0.24572, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.2487 - loss: 3.5576 - val_accuracy: 0.0642 - val_loss: 12.9870\n",
            "Epoch 53/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2535 - loss: 3.5066\n",
            "Epoch 53: accuracy improved from 0.24572 to 0.24806, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.2534 - loss: 3.5069 - val_accuracy: 0.0642 - val_loss: 12.7882\n",
            "Epoch 54/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2575 - loss: 3.4863\n",
            "Epoch 54: accuracy improved from 0.24806 to 0.25137, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.2575 - loss: 3.4866 - val_accuracy: 0.0653 - val_loss: 13.3303\n",
            "Epoch 55/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2643 - loss: 3.4210\n",
            "Epoch 55: accuracy improved from 0.25137 to 0.25864, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.2643 - loss: 3.4212 - val_accuracy: 0.0642 - val_loss: 13.5555\n",
            "Epoch 56/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.2652 - loss: 3.4069\n",
            "Epoch 56: accuracy improved from 0.25864 to 0.26104, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.2652 - loss: 3.4073 - val_accuracy: 0.0645 - val_loss: 13.8154\n",
            "Epoch 57/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.2692 - loss: 3.3753\n",
            "Epoch 57: accuracy improved from 0.26104 to 0.26513, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.2691 - loss: 3.3755 - val_accuracy: 0.0639 - val_loss: 14.0635\n",
            "Epoch 58/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2763 - loss: 3.3407\n",
            "Epoch 58: accuracy improved from 0.26513 to 0.27284, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.2763 - loss: 3.3408 - val_accuracy: 0.0620 - val_loss: 14.1802\n",
            "Epoch 59/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2767 - loss: 3.3018\n",
            "Epoch 59: accuracy improved from 0.27284 to 0.27412, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.2767 - loss: 3.3020 - val_accuracy: 0.0658 - val_loss: 14.3996\n",
            "Epoch 60/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.2861 - loss: 3.2538\n",
            "Epoch 60: accuracy improved from 0.27412 to 0.27646, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.2860 - loss: 3.2544 - val_accuracy: 0.0619 - val_loss: 14.0933\n",
            "Epoch 61/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2858 - loss: 3.2482\n",
            "Epoch 61: accuracy improved from 0.27646 to 0.28230, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.2858 - loss: 3.2486 - val_accuracy: 0.0618 - val_loss: 14.3271\n",
            "Epoch 62/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.2932 - loss: 3.2079\n",
            "Epoch 62: accuracy improved from 0.28230 to 0.28489, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.2931 - loss: 3.2085 - val_accuracy: 0.0602 - val_loss: 14.4783\n",
            "Epoch 63/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2966 - loss: 3.1687\n",
            "Epoch 63: accuracy improved from 0.28489 to 0.28767, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.2965 - loss: 3.1689 - val_accuracy: 0.0589 - val_loss: 14.3040\n",
            "Epoch 64/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2936 - loss: 3.1878\n",
            "Epoch 64: accuracy improved from 0.28767 to 0.28926, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.2936 - loss: 3.1880 - val_accuracy: 0.0618 - val_loss: 14.7552\n",
            "Epoch 65/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3020 - loss: 3.1215\n",
            "Epoch 65: accuracy improved from 0.28926 to 0.29119, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.3019 - loss: 3.1223 - val_accuracy: 0.0625 - val_loss: 14.5512\n",
            "Epoch 66/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2994 - loss: 3.1560\n",
            "Epoch 66: accuracy improved from 0.29119 to 0.29519, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.2994 - loss: 3.1561 - val_accuracy: 0.0622 - val_loss: 15.2622\n",
            "Epoch 67/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3065 - loss: 3.0809\n",
            "Epoch 67: accuracy improved from 0.29519 to 0.30281, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.3065 - loss: 3.0810 - val_accuracy: 0.0622 - val_loss: 15.6419\n",
            "Epoch 68/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3122 - loss: 3.0387\n",
            "Epoch 68: accuracy improved from 0.30281 to 0.30817, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.3121 - loss: 3.0392 - val_accuracy: 0.0612 - val_loss: 15.6725\n",
            "Epoch 69/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.3159 - loss: 3.0038\n",
            "Epoch 69: accuracy improved from 0.30817 to 0.31251, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - accuracy: 0.3159 - loss: 3.0039 - val_accuracy: 0.0615 - val_loss: 15.7858\n",
            "Epoch 70/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3238 - loss: 2.9635\n",
            "Epoch 70: accuracy improved from 0.31251 to 0.31492, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.3237 - loss: 2.9642 - val_accuracy: 0.0637 - val_loss: 15.9596\n",
            "Epoch 71/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3327 - loss: 2.9325\n",
            "Epoch 71: accuracy improved from 0.31492 to 0.32094, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.3326 - loss: 2.9332 - val_accuracy: 0.0615 - val_loss: 15.6961\n",
            "Epoch 72/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3265 - loss: 2.9385\n",
            "Epoch 72: accuracy improved from 0.32094 to 0.32104, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - accuracy: 0.3265 - loss: 2.9387 - val_accuracy: 0.0620 - val_loss: 15.9418\n",
            "Epoch 73/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3381 - loss: 2.8800\n",
            "Epoch 73: accuracy improved from 0.32104 to 0.33015, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.3380 - loss: 2.8803 - val_accuracy: 0.0629 - val_loss: 16.6539\n",
            "Epoch 74/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3428 - loss: 2.8344\n",
            "Epoch 74: accuracy improved from 0.33015 to 0.33555, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.3428 - loss: 2.8346 - val_accuracy: 0.0592 - val_loss: 16.7977\n",
            "Epoch 75/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3460 - loss: 2.8160\n",
            "Epoch 75: accuracy improved from 0.33555 to 0.33845, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.3459 - loss: 2.8165 - val_accuracy: 0.0612 - val_loss: 16.7156\n",
            "Epoch 76/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3479 - loss: 2.7932\n",
            "Epoch 76: accuracy improved from 0.33845 to 0.34223, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.3479 - loss: 2.7935 - val_accuracy: 0.0604 - val_loss: 17.0929\n",
            "Epoch 77/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3514 - loss: 2.7731\n",
            "Epoch 77: accuracy improved from 0.34223 to 0.34644, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.3514 - loss: 2.7736 - val_accuracy: 0.0594 - val_loss: 17.0969\n",
            "Epoch 78/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3578 - loss: 2.7560\n",
            "Epoch 78: accuracy improved from 0.34644 to 0.35428, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.3578 - loss: 2.7563 - val_accuracy: 0.0635 - val_loss: 17.4038\n",
            "Epoch 79/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3618 - loss: 2.7251\n",
            "Epoch 79: accuracy improved from 0.35428 to 0.35528, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.3618 - loss: 2.7252 - val_accuracy: 0.0604 - val_loss: 17.4826\n",
            "Epoch 80/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3654 - loss: 2.6829\n",
            "Epoch 80: accuracy improved from 0.35528 to 0.35899, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.3654 - loss: 2.6833 - val_accuracy: 0.0592 - val_loss: 17.3478\n",
            "Epoch 81/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3787 - loss: 2.6447\n",
            "Epoch 81: accuracy improved from 0.35899 to 0.36829, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.3786 - loss: 2.6451 - val_accuracy: 0.0593 - val_loss: 17.9518\n",
            "Epoch 82/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3770 - loss: 2.6302\n",
            "Epoch 82: accuracy did not improve from 0.36829\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.3769 - loss: 2.6309 - val_accuracy: 0.0597 - val_loss: 17.7274\n",
            "Epoch 83/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3850 - loss: 2.6049\n",
            "Epoch 83: accuracy improved from 0.36829 to 0.37432, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.3849 - loss: 2.6054 - val_accuracy: 0.0603 - val_loss: 18.0254\n",
            "Epoch 84/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3843 - loss: 2.5991\n",
            "Epoch 84: accuracy improved from 0.37432 to 0.37591, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.3842 - loss: 2.5997 - val_accuracy: 0.0583 - val_loss: 17.9621\n",
            "Epoch 85/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3885 - loss: 2.5554\n",
            "Epoch 85: accuracy improved from 0.37591 to 0.38131, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.3884 - loss: 2.5560 - val_accuracy: 0.0598 - val_loss: 18.2772\n",
            "Epoch 86/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3910 - loss: 2.5259\n",
            "Epoch 86: accuracy improved from 0.38131 to 0.38296, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.3909 - loss: 2.5265 - val_accuracy: 0.0607 - val_loss: 18.4668\n",
            "Epoch 87/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4051 - loss: 2.4747\n",
            "Epoch 87: accuracy improved from 0.38296 to 0.38911, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.4050 - loss: 2.4753 - val_accuracy: 0.0599 - val_loss: 18.2519\n",
            "Epoch 88/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4035 - loss: 2.4663\n",
            "Epoch 88: accuracy improved from 0.38911 to 0.39348, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.4034 - loss: 2.4667 - val_accuracy: 0.0609 - val_loss: 18.4407\n",
            "Epoch 89/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4078 - loss: 2.4476\n",
            "Epoch 89: accuracy improved from 0.39348 to 0.39629, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.4076 - loss: 2.4483 - val_accuracy: 0.0572 - val_loss: 18.6343\n",
            "Epoch 90/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4121 - loss: 2.4223\n",
            "Epoch 90: accuracy improved from 0.39629 to 0.40057, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.4119 - loss: 2.4230 - val_accuracy: 0.0581 - val_loss: 19.0108\n",
            "Epoch 91/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4162 - loss: 2.4233\n",
            "Epoch 91: accuracy improved from 0.40057 to 0.40853, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.4161 - loss: 2.4233 - val_accuracy: 0.0603 - val_loss: 19.4155\n",
            "Epoch 92/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4192 - loss: 2.3830\n",
            "Epoch 92: accuracy improved from 0.40853 to 0.40928, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.4190 - loss: 2.3836 - val_accuracy: 0.0593 - val_loss: 19.5434\n",
            "Epoch 93/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4219 - loss: 2.3791\n",
            "Epoch 93: accuracy improved from 0.40928 to 0.41352, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.4218 - loss: 2.3796 - val_accuracy: 0.0591 - val_loss: 19.0398\n",
            "Epoch 94/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4244 - loss: 2.3664\n",
            "Epoch 94: accuracy improved from 0.41352 to 0.41618, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.4244 - loss: 2.3666 - val_accuracy: 0.0615 - val_loss: 19.2986\n",
            "Epoch 95/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4275 - loss: 2.3410\n",
            "Epoch 95: accuracy improved from 0.41618 to 0.42042, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.4274 - loss: 2.3414 - val_accuracy: 0.0607 - val_loss: 19.9127\n",
            "Epoch 96/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4339 - loss: 2.3081\n",
            "Epoch 96: accuracy improved from 0.42042 to 0.42182, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.4337 - loss: 2.3087 - val_accuracy: 0.0597 - val_loss: 19.7023\n",
            "Epoch 97/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4254 - loss: 2.3301\n",
            "Epoch 97: accuracy did not improve from 0.42182\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.4253 - loss: 2.3307 - val_accuracy: 0.0598 - val_loss: 19.7524\n",
            "Epoch 98/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4327 - loss: 2.3392\n",
            "Epoch 98: accuracy did not improve from 0.42182\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.4326 - loss: 2.3395 - val_accuracy: 0.0603 - val_loss: 19.1793\n",
            "Epoch 99/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4438 - loss: 2.2730\n",
            "Epoch 99: accuracy improved from 0.42182 to 0.43287, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.4437 - loss: 2.2734 - val_accuracy: 0.0614 - val_loss: 19.9882\n",
            "Epoch 100/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4455 - loss: 2.2625\n",
            "Epoch 100: accuracy improved from 0.43287 to 0.43724, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.4455 - loss: 2.2626 - val_accuracy: 0.0599 - val_loss: 20.1862\n",
            "Epoch 101/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4457 - loss: 2.2442\n",
            "Epoch 101: accuracy improved from 0.43724 to 0.44080, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.4456 - loss: 2.2443 - val_accuracy: 0.0613 - val_loss: 19.8824\n",
            "Epoch 102/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4552 - loss: 2.1954\n",
            "Epoch 102: accuracy improved from 0.44080 to 0.44680, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.4551 - loss: 2.1959 - val_accuracy: 0.0588 - val_loss: 20.4061\n",
            "Epoch 103/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4556 - loss: 2.1834\n",
            "Epoch 103: accuracy did not improve from 0.44680\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.4555 - loss: 2.1838 - val_accuracy: 0.0587 - val_loss: 20.3976\n",
            "Epoch 104/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4588 - loss: 2.1752\n",
            "Epoch 104: accuracy improved from 0.44680 to 0.45238, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.4588 - loss: 2.1753 - val_accuracy: 0.0576 - val_loss: 20.7296\n",
            "Epoch 105/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4659 - loss: 2.1305\n",
            "Epoch 105: accuracy improved from 0.45238 to 0.45519, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.4658 - loss: 2.1309 - val_accuracy: 0.0599 - val_loss: 20.7681\n",
            "Epoch 106/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.4685 - loss: 2.1447\n",
            "Epoch 106: accuracy improved from 0.45519 to 0.46468, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.4685 - loss: 2.1449 - val_accuracy: 0.0604 - val_loss: 20.9779\n",
            "Epoch 107/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4784 - loss: 2.0847\n",
            "Epoch 107: accuracy did not improve from 0.46468\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.4783 - loss: 2.0852 - val_accuracy: 0.0572 - val_loss: 20.7502\n",
            "Epoch 108/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4707 - loss: 2.0982\n",
            "Epoch 108: accuracy improved from 0.46468 to 0.46721, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.4707 - loss: 2.0985 - val_accuracy: 0.0600 - val_loss: 21.2347\n",
            "Epoch 109/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4766 - loss: 2.0861\n",
            "Epoch 109: accuracy improved from 0.46721 to 0.47030, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.4766 - loss: 2.0862 - val_accuracy: 0.0607 - val_loss: 21.1392\n",
            "Epoch 110/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4875 - loss: 2.0506\n",
            "Epoch 110: accuracy improved from 0.47030 to 0.47554, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.4874 - loss: 2.0510 - val_accuracy: 0.0602 - val_loss: 21.1492\n",
            "Epoch 111/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4819 - loss: 2.0368\n",
            "Epoch 111: accuracy did not improve from 0.47554\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.4819 - loss: 2.0373 - val_accuracy: 0.0584 - val_loss: 21.0610\n",
            "Epoch 112/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4872 - loss: 2.0351\n",
            "Epoch 112: accuracy improved from 0.47554 to 0.47970, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.4871 - loss: 2.0353 - val_accuracy: 0.0614 - val_loss: 21.4741\n",
            "Epoch 113/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.4949 - loss: 1.9982\n",
            "Epoch 113: accuracy improved from 0.47970 to 0.48575, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.4949 - loss: 1.9984 - val_accuracy: 0.0604 - val_loss: 21.5050\n",
            "Epoch 114/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.4973 - loss: 1.9856\n",
            "Epoch 114: accuracy improved from 0.48575 to 0.48847, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.4972 - loss: 1.9857 - val_accuracy: 0.0619 - val_loss: 21.7946\n",
            "Epoch 115/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5072 - loss: 1.9464\n",
            "Epoch 115: accuracy improved from 0.48847 to 0.49418, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.5070 - loss: 1.9471 - val_accuracy: 0.0617 - val_loss: 21.4119\n",
            "Epoch 116/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5069 - loss: 1.9460\n",
            "Epoch 116: accuracy improved from 0.49418 to 0.49661, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.5068 - loss: 1.9464 - val_accuracy: 0.0604 - val_loss: 21.7783\n",
            "Epoch 117/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5073 - loss: 1.9286\n",
            "Epoch 117: accuracy did not improve from 0.49661\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.5071 - loss: 1.9291 - val_accuracy: 0.0605 - val_loss: 22.0322\n",
            "Epoch 118/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5077 - loss: 1.9143\n",
            "Epoch 118: accuracy improved from 0.49661 to 0.49899, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.5076 - loss: 1.9146 - val_accuracy: 0.0592 - val_loss: 22.1112\n",
            "Epoch 119/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5124 - loss: 1.9202\n",
            "Epoch 119: accuracy improved from 0.49899 to 0.50317, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.5123 - loss: 1.9204 - val_accuracy: 0.0582 - val_loss: 21.6864\n",
            "Epoch 120/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5199 - loss: 1.8869\n",
            "Epoch 120: accuracy improved from 0.50317 to 0.50748, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.5198 - loss: 1.8871 - val_accuracy: 0.0610 - val_loss: 22.0746\n",
            "Epoch 121/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5159 - loss: 1.8821\n",
            "Epoch 121: accuracy did not improve from 0.50748\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.5158 - loss: 1.8825 - val_accuracy: 0.0600 - val_loss: 21.8605\n",
            "Epoch 122/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5272 - loss: 1.8467\n",
            "Epoch 122: accuracy improved from 0.50748 to 0.51472, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.5271 - loss: 1.8472 - val_accuracy: 0.0612 - val_loss: 21.8542\n",
            "Epoch 123/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5253 - loss: 1.8647\n",
            "Epoch 123: accuracy improved from 0.51472 to 0.51859, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.5252 - loss: 1.8648 - val_accuracy: 0.0614 - val_loss: 22.3965\n",
            "Epoch 124/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5318 - loss: 1.8108\n",
            "Epoch 124: accuracy improved from 0.51859 to 0.51937, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.5318 - loss: 1.8111 - val_accuracy: 0.0591 - val_loss: 21.7681\n",
            "Epoch 125/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5281 - loss: 1.8465\n",
            "Epoch 125: accuracy improved from 0.51937 to 0.52586, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.5281 - loss: 1.8466 - val_accuracy: 0.0586 - val_loss: 22.2632\n",
            "Epoch 126/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5319 - loss: 1.8045\n",
            "Epoch 126: accuracy did not improve from 0.52586\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.5318 - loss: 1.8047 - val_accuracy: 0.0603 - val_loss: 22.5023\n",
            "Epoch 127/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5396 - loss: 1.7874\n",
            "Epoch 127: accuracy improved from 0.52586 to 0.52708, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.5395 - loss: 1.7878 - val_accuracy: 0.0597 - val_loss: 22.1385\n",
            "Epoch 128/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5346 - loss: 1.8025\n",
            "Epoch 128: accuracy improved from 0.52708 to 0.52811, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.5345 - loss: 1.8027 - val_accuracy: 0.0600 - val_loss: 22.8101\n",
            "Epoch 129/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5435 - loss: 1.7785\n",
            "Epoch 129: accuracy improved from 0.52811 to 0.53189, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.5435 - loss: 1.7786 - val_accuracy: 0.0588 - val_loss: 22.6823\n",
            "Epoch 130/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5430 - loss: 1.7460\n",
            "Epoch 130: accuracy improved from 0.53189 to 0.53451, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.5429 - loss: 1.7465 - val_accuracy: 0.0604 - val_loss: 22.8488\n",
            "Epoch 131/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5492 - loss: 1.7259\n",
            "Epoch 131: accuracy improved from 0.53451 to 0.53969, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.5491 - loss: 1.7263 - val_accuracy: 0.0589 - val_loss: 22.7301\n",
            "Epoch 132/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5458 - loss: 1.7703\n",
            "Epoch 132: accuracy did not improve from 0.53969\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.5458 - loss: 1.7704 - val_accuracy: 0.0566 - val_loss: 22.9997\n",
            "Epoch 133/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5492 - loss: 1.7213\n",
            "Epoch 133: accuracy improved from 0.53969 to 0.54343, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.5492 - loss: 1.7216 - val_accuracy: 0.0581 - val_loss: 23.2652\n",
            "Epoch 134/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5596 - loss: 1.6813\n",
            "Epoch 134: accuracy improved from 0.54343 to 0.54805, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.5596 - loss: 1.6814 - val_accuracy: 0.0612 - val_loss: 23.2352\n",
            "Epoch 135/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5603 - loss: 1.6795\n",
            "Epoch 135: accuracy improved from 0.54805 to 0.54840, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.5603 - loss: 1.6797 - val_accuracy: 0.0605 - val_loss: 23.1028\n",
            "Epoch 136/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5582 - loss: 1.6816\n",
            "Epoch 136: accuracy improved from 0.54840 to 0.54940, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.5581 - loss: 1.6818 - val_accuracy: 0.0591 - val_loss: 23.2441\n",
            "Epoch 137/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5642 - loss: 1.6752\n",
            "Epoch 137: accuracy improved from 0.54940 to 0.55508, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.5641 - loss: 1.6754 - val_accuracy: 0.0581 - val_loss: 23.3711\n",
            "Epoch 138/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5665 - loss: 1.6585\n",
            "Epoch 138: accuracy improved from 0.55508 to 0.55913, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.5664 - loss: 1.6588 - val_accuracy: 0.0593 - val_loss: 23.5559\n",
            "Epoch 139/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5710 - loss: 1.6426\n",
            "Epoch 139: accuracy did not improve from 0.55913\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.5709 - loss: 1.6431 - val_accuracy: 0.0605 - val_loss: 23.3351\n",
            "Epoch 140/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5742 - loss: 1.6256\n",
            "Epoch 140: accuracy improved from 0.55913 to 0.56556, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.5741 - loss: 1.6259 - val_accuracy: 0.0602 - val_loss: 23.4215\n",
            "Epoch 141/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5707 - loss: 1.6421\n",
            "Epoch 141: accuracy did not improve from 0.56556\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.5706 - loss: 1.6424 - val_accuracy: 0.0618 - val_loss: 23.8543\n",
            "Epoch 142/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5739 - loss: 1.6135\n",
            "Epoch 142: accuracy improved from 0.56556 to 0.56859, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.5739 - loss: 1.6140 - val_accuracy: 0.0614 - val_loss: 23.6372\n",
            "Epoch 143/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5765 - loss: 1.5992\n",
            "Epoch 143: accuracy did not improve from 0.56859\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.5765 - loss: 1.5994 - val_accuracy: 0.0629 - val_loss: 23.8071\n",
            "Epoch 144/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5729 - loss: 1.6093\n",
            "Epoch 144: accuracy improved from 0.56859 to 0.57178, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.5729 - loss: 1.6094 - val_accuracy: 0.0612 - val_loss: 23.9528\n",
            "Epoch 145/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5855 - loss: 1.5675\n",
            "Epoch 145: accuracy improved from 0.57178 to 0.57849, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.5854 - loss: 1.5678 - val_accuracy: 0.0608 - val_loss: 24.3107\n",
            "Epoch 146/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5836 - loss: 1.5782\n",
            "Epoch 146: accuracy did not improve from 0.57849\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.5836 - loss: 1.5784 - val_accuracy: 0.0594 - val_loss: 24.2413\n",
            "Epoch 147/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5732 - loss: 1.6002\n",
            "Epoch 147: accuracy did not improve from 0.57849\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.5732 - loss: 1.6002 - val_accuracy: 0.0610 - val_loss: 24.3032\n",
            "Epoch 148/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5843 - loss: 1.5640\n",
            "Epoch 148: accuracy did not improve from 0.57849\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.5842 - loss: 1.5643 - val_accuracy: 0.0592 - val_loss: 24.0578\n",
            "Epoch 149/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5990 - loss: 1.5400\n",
            "Epoch 149: accuracy improved from 0.57849 to 0.58776, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.5989 - loss: 1.5403 - val_accuracy: 0.0607 - val_loss: 24.2277\n",
            "Epoch 150/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5925 - loss: 1.5197\n",
            "Epoch 150: accuracy did not improve from 0.58776\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.5925 - loss: 1.5198 - val_accuracy: 0.0623 - val_loss: 24.4205\n",
            "Epoch 151/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5948 - loss: 1.5408\n",
            "Epoch 151: accuracy did not improve from 0.58776\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.5947 - loss: 1.5410 - val_accuracy: 0.0605 - val_loss: 24.0825\n",
            "Epoch 152/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5942 - loss: 1.5390\n",
            "Epoch 152: accuracy improved from 0.58776 to 0.58876, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.5941 - loss: 1.5391 - val_accuracy: 0.0600 - val_loss: 24.1482\n",
            "Epoch 153/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6068 - loss: 1.4719\n",
            "Epoch 153: accuracy improved from 0.58876 to 0.59831, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.6067 - loss: 1.4724 - val_accuracy: 0.0588 - val_loss: 24.6930\n",
            "Epoch 154/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6056 - loss: 1.4749\n",
            "Epoch 154: accuracy improved from 0.59831 to 0.59965, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.6055 - loss: 1.4751 - val_accuracy: 0.0594 - val_loss: 25.0157\n",
            "Epoch 155/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6098 - loss: 1.4747\n",
            "Epoch 155: accuracy improved from 0.59965 to 0.60202, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.6098 - loss: 1.4749 - val_accuracy: 0.0612 - val_loss: 24.4759\n",
            "Epoch 156/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6044 - loss: 1.4669\n",
            "Epoch 156: accuracy did not improve from 0.60202\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.6044 - loss: 1.4672 - val_accuracy: 0.0578 - val_loss: 24.1168\n",
            "Epoch 157/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6089 - loss: 1.4709\n",
            "Epoch 157: accuracy improved from 0.60202 to 0.60277, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.6089 - loss: 1.4710 - val_accuracy: 0.0592 - val_loss: 24.6265\n",
            "Epoch 158/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6123 - loss: 1.4519\n",
            "Epoch 158: accuracy did not improve from 0.60277\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.6123 - loss: 1.4521 - val_accuracy: 0.0619 - val_loss: 24.9401\n",
            "Epoch 159/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6159 - loss: 1.4322\n",
            "Epoch 159: accuracy improved from 0.60277 to 0.60699, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.6158 - loss: 1.4325 - val_accuracy: 0.0599 - val_loss: 24.8529\n",
            "Epoch 160/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6198 - loss: 1.4225\n",
            "Epoch 160: accuracy improved from 0.60699 to 0.61076, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.6197 - loss: 1.4228 - val_accuracy: 0.0604 - val_loss: 24.6592\n",
            "Epoch 161/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6148 - loss: 1.4413\n",
            "Epoch 161: accuracy improved from 0.61076 to 0.61235, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.6148 - loss: 1.4414 - val_accuracy: 0.0587 - val_loss: 25.2953\n",
            "Epoch 162/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6235 - loss: 1.4110\n",
            "Epoch 162: accuracy improved from 0.61235 to 0.61360, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 26ms/step - accuracy: 0.6234 - loss: 1.4113 - val_accuracy: 0.0598 - val_loss: 24.7460\n",
            "Epoch 163/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6237 - loss: 1.4058\n",
            "Epoch 163: accuracy improved from 0.61360 to 0.61504, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.6237 - loss: 1.4060 - val_accuracy: 0.0561 - val_loss: 24.8641\n",
            "Epoch 164/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6254 - loss: 1.4022\n",
            "Epoch 164: accuracy improved from 0.61504 to 0.61857, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.6253 - loss: 1.4025 - val_accuracy: 0.0596 - val_loss: 24.8397\n",
            "Epoch 165/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6265 - loss: 1.3874\n",
            "Epoch 165: accuracy did not improve from 0.61857\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.6264 - loss: 1.3878 - val_accuracy: 0.0596 - val_loss: 24.7459\n",
            "Epoch 166/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6300 - loss: 1.3748\n",
            "Epoch 166: accuracy improved from 0.61857 to 0.62365, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 28ms/step - accuracy: 0.6299 - loss: 1.3750 - val_accuracy: 0.0598 - val_loss: 25.2017\n",
            "Epoch 167/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6351 - loss: 1.3615\n",
            "Epoch 167: accuracy improved from 0.62365 to 0.62543, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.6350 - loss: 1.3616 - val_accuracy: 0.0597 - val_loss: 25.0173\n",
            "Epoch 168/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6362 - loss: 1.3646\n",
            "Epoch 168: accuracy improved from 0.62543 to 0.62880, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.6361 - loss: 1.3647 - val_accuracy: 0.0607 - val_loss: 25.3107\n",
            "Epoch 169/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6282 - loss: 1.3659\n",
            "Epoch 169: accuracy did not improve from 0.62880\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.6282 - loss: 1.3661 - val_accuracy: 0.0620 - val_loss: 25.3168\n",
            "Epoch 170/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6377 - loss: 1.3541\n",
            "Epoch 170: accuracy improved from 0.62880 to 0.62893, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.6376 - loss: 1.3544 - val_accuracy: 0.0628 - val_loss: 25.1874\n",
            "Epoch 171/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6359 - loss: 1.3367\n",
            "Epoch 171: accuracy did not improve from 0.62893\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.6358 - loss: 1.3370 - val_accuracy: 0.0607 - val_loss: 25.1807\n",
            "Epoch 172/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6416 - loss: 1.3341\n",
            "Epoch 172: accuracy improved from 0.62893 to 0.63292, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.6415 - loss: 1.3344 - val_accuracy: 0.0592 - val_loss: 25.4182\n",
            "Epoch 173/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6420 - loss: 1.3270\n",
            "Epoch 173: accuracy improved from 0.63292 to 0.63439, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.6420 - loss: 1.3272 - val_accuracy: 0.0600 - val_loss: 25.2807\n",
            "Epoch 174/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6393 - loss: 1.3313\n",
            "Epoch 174: accuracy improved from 0.63439 to 0.63561, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.6393 - loss: 1.3314 - val_accuracy: 0.0603 - val_loss: 25.4478\n",
            "Epoch 175/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6424 - loss: 1.3069\n",
            "Epoch 175: accuracy did not improve from 0.63561\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.6424 - loss: 1.3071 - val_accuracy: 0.0582 - val_loss: 25.5658\n",
            "Epoch 176/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6470 - loss: 1.3154\n",
            "Epoch 176: accuracy improved from 0.63561 to 0.63967, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.6469 - loss: 1.3158 - val_accuracy: 0.0586 - val_loss: 25.1113\n",
            "Epoch 177/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6531 - loss: 1.2701\n",
            "Epoch 177: accuracy improved from 0.63967 to 0.64516, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.6530 - loss: 1.2704 - val_accuracy: 0.0589 - val_loss: 25.6972\n",
            "Epoch 178/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6510 - loss: 1.2829\n",
            "Epoch 178: accuracy did not improve from 0.64516\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.6510 - loss: 1.2830 - val_accuracy: 0.0603 - val_loss: 26.0772\n",
            "Epoch 179/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6498 - loss: 1.2986\n",
            "Epoch 179: accuracy did not improve from 0.64516\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.6497 - loss: 1.2987 - val_accuracy: 0.0596 - val_loss: 25.6247\n",
            "Epoch 180/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6493 - loss: 1.2871\n",
            "Epoch 180: accuracy did not improve from 0.64516\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.6493 - loss: 1.2873 - val_accuracy: 0.0634 - val_loss: 25.6393\n",
            "Epoch 181/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6572 - loss: 1.2621\n",
            "Epoch 181: accuracy improved from 0.64516 to 0.64653, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.6571 - loss: 1.2624 - val_accuracy: 0.0602 - val_loss: 25.4703\n",
            "Epoch 182/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6576 - loss: 1.2636\n",
            "Epoch 182: accuracy improved from 0.64653 to 0.64660, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.6576 - loss: 1.2637 - val_accuracy: 0.0596 - val_loss: 25.8854\n",
            "Epoch 183/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6622 - loss: 1.2528\n",
            "Epoch 183: accuracy improved from 0.64660 to 0.65396, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.6621 - loss: 1.2531 - val_accuracy: 0.0591 - val_loss: 25.5542\n",
            "Epoch 184/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6634 - loss: 1.2313\n",
            "Epoch 184: accuracy did not improve from 0.65396\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.6632 - loss: 1.2316 - val_accuracy: 0.0594 - val_loss: 26.1824\n",
            "Epoch 185/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6664 - loss: 1.2224\n",
            "Epoch 185: accuracy improved from 0.65396 to 0.66011, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.6664 - loss: 1.2227 - val_accuracy: 0.0597 - val_loss: 25.7492\n",
            "Epoch 186/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6700 - loss: 1.2064\n",
            "Epoch 186: accuracy did not improve from 0.66011\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.6699 - loss: 1.2069 - val_accuracy: 0.0599 - val_loss: 26.0089\n",
            "Epoch 187/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6559 - loss: 1.2506\n",
            "Epoch 187: accuracy did not improve from 0.66011\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.6559 - loss: 1.2507 - val_accuracy: 0.0572 - val_loss: 25.6206\n",
            "Epoch 188/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6653 - loss: 1.2024\n",
            "Epoch 188: accuracy did not improve from 0.66011\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.6653 - loss: 1.2028 - val_accuracy: 0.0608 - val_loss: 26.1972\n",
            "Epoch 189/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6746 - loss: 1.2040\n",
            "Epoch 189: accuracy improved from 0.66011 to 0.66264, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.6746 - loss: 1.2041 - val_accuracy: 0.0597 - val_loss: 25.7235\n",
            "Epoch 190/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6768 - loss: 1.1782\n",
            "Epoch 190: accuracy improved from 0.66264 to 0.66707, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.6767 - loss: 1.1785 - val_accuracy: 0.0588 - val_loss: 26.1799\n",
            "Epoch 191/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6762 - loss: 1.2009\n",
            "Epoch 191: accuracy did not improve from 0.66707\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.6761 - loss: 1.2011 - val_accuracy: 0.0604 - val_loss: 26.1557\n",
            "Epoch 192/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6792 - loss: 1.1773\n",
            "Epoch 192: accuracy improved from 0.66707 to 0.66916, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.6791 - loss: 1.1778 - val_accuracy: 0.0589 - val_loss: 25.9648\n",
            "Epoch 193/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6731 - loss: 1.1940\n",
            "Epoch 193: accuracy did not improve from 0.66916\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.6731 - loss: 1.1941 - val_accuracy: 0.0603 - val_loss: 26.3770\n",
            "Epoch 194/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6743 - loss: 1.1892\n",
            "Epoch 194: accuracy did not improve from 0.66916\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.6742 - loss: 1.1896 - val_accuracy: 0.0592 - val_loss: 26.2631\n",
            "Epoch 195/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6770 - loss: 1.1709\n",
            "Epoch 195: accuracy improved from 0.66916 to 0.67179, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.6770 - loss: 1.1711 - val_accuracy: 0.0587 - val_loss: 26.9163\n",
            "Epoch 196/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6734 - loss: 1.1847\n",
            "Epoch 196: accuracy did not improve from 0.67179\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.6733 - loss: 1.1848 - val_accuracy: 0.0608 - val_loss: 26.4046\n",
            "Epoch 197/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6814 - loss: 1.1626\n",
            "Epoch 197: accuracy improved from 0.67179 to 0.67428, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.6813 - loss: 1.1629 - val_accuracy: 0.0597 - val_loss: 26.2418\n",
            "Epoch 198/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6782 - loss: 1.1637\n",
            "Epoch 198: accuracy improved from 0.67428 to 0.67478, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.6782 - loss: 1.1638 - val_accuracy: 0.0591 - val_loss: 26.6844\n",
            "Epoch 199/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6805 - loss: 1.1610\n",
            "Epoch 199: accuracy did not improve from 0.67478\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.6805 - loss: 1.1613 - val_accuracy: 0.0567 - val_loss: 26.6355\n",
            "Epoch 200/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6818 - loss: 1.1524\n",
            "Epoch 200: accuracy improved from 0.67478 to 0.67587, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.6818 - loss: 1.1526 - val_accuracy: 0.0578 - val_loss: 26.2427\n",
            "Epoch 201/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6896 - loss: 1.1263\n",
            "Epoch 201: accuracy improved from 0.67587 to 0.68140, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.6896 - loss: 1.1264 - val_accuracy: 0.0577 - val_loss: 27.0517\n",
            "Epoch 202/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6886 - loss: 1.1360\n",
            "Epoch 202: accuracy improved from 0.68140 to 0.68262, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.6885 - loss: 1.1363 - val_accuracy: 0.0582 - val_loss: 26.6294\n",
            "Epoch 203/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6950 - loss: 1.1033\n",
            "Epoch 203: accuracy improved from 0.68262 to 0.68777, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.6949 - loss: 1.1037 - val_accuracy: 0.0599 - val_loss: 26.5735\n",
            "Epoch 204/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6921 - loss: 1.1203\n",
            "Epoch 204: accuracy did not improve from 0.68777\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.6920 - loss: 1.1204 - val_accuracy: 0.0592 - val_loss: 26.7753\n",
            "Epoch 205/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6953 - loss: 1.1148\n",
            "Epoch 205: accuracy improved from 0.68777 to 0.68824, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.6953 - loss: 1.1150 - val_accuracy: 0.0598 - val_loss: 26.6444\n",
            "Epoch 206/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7007 - loss: 1.0980\n",
            "Epoch 206: accuracy improved from 0.68824 to 0.69011, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.7006 - loss: 1.0982 - val_accuracy: 0.0579 - val_loss: 27.3705\n",
            "Epoch 207/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6928 - loss: 1.0957\n",
            "Epoch 207: accuracy did not improve from 0.69011\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.6927 - loss: 1.0961 - val_accuracy: 0.0581 - val_loss: 26.9229\n",
            "Epoch 208/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.6920 - loss: 1.1165\n",
            "Epoch 208: accuracy did not improve from 0.69011\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.6919 - loss: 1.1167 - val_accuracy: 0.0581 - val_loss: 27.1730\n",
            "Epoch 209/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6965 - loss: 1.0972\n",
            "Epoch 209: accuracy did not improve from 0.69011\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.6965 - loss: 1.0974 - val_accuracy: 0.0577 - val_loss: 27.1955\n",
            "Epoch 210/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7035 - loss: 1.0831\n",
            "Epoch 210: accuracy improved from 0.69011 to 0.69551, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7035 - loss: 1.0832 - val_accuracy: 0.0562 - val_loss: 26.7958\n",
            "Epoch 211/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.6983 - loss: 1.0822\n",
            "Epoch 211: accuracy did not improve from 0.69551\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.6982 - loss: 1.0824 - val_accuracy: 0.0571 - val_loss: 27.1351\n",
            "Epoch 212/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7002 - loss: 1.0882\n",
            "Epoch 212: accuracy did not improve from 0.69551\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7001 - loss: 1.0884 - val_accuracy: 0.0603 - val_loss: 27.3248\n",
            "Epoch 213/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7038 - loss: 1.0752\n",
            "Epoch 213: accuracy improved from 0.69551 to 0.69591, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7037 - loss: 1.0755 - val_accuracy: 0.0571 - val_loss: 27.1453\n",
            "Epoch 214/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7098 - loss: 1.0559\n",
            "Epoch 214: accuracy improved from 0.69591 to 0.69972, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.7097 - loss: 1.0560 - val_accuracy: 0.0574 - val_loss: 27.0453\n",
            "Epoch 215/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7030 - loss: 1.0867\n",
            "Epoch 215: accuracy did not improve from 0.69972\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7029 - loss: 1.0868 - val_accuracy: 0.0592 - val_loss: 27.1383\n",
            "Epoch 216/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7117 - loss: 1.0454\n",
            "Epoch 216: accuracy improved from 0.69972 to 0.70175, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7116 - loss: 1.0456 - val_accuracy: 0.0604 - val_loss: 27.3815\n",
            "Epoch 217/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7103 - loss: 1.0431\n",
            "Epoch 217: accuracy improved from 0.70175 to 0.70575, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.7103 - loss: 1.0433 - val_accuracy: 0.0592 - val_loss: 27.7030\n",
            "Epoch 218/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7119 - loss: 1.0417\n",
            "Epoch 218: accuracy did not improve from 0.70575\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7118 - loss: 1.0420 - val_accuracy: 0.0600 - val_loss: 28.2087\n",
            "Epoch 219/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7128 - loss: 1.0479\n",
            "Epoch 219: accuracy did not improve from 0.70575\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7127 - loss: 1.0481 - val_accuracy: 0.0593 - val_loss: 27.5001\n",
            "Epoch 220/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7101 - loss: 1.0521\n",
            "Epoch 220: accuracy did not improve from 0.70575\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7100 - loss: 1.0523 - val_accuracy: 0.0579 - val_loss: 27.7568\n",
            "Epoch 221/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7122 - loss: 1.0389\n",
            "Epoch 221: accuracy improved from 0.70575 to 0.70924, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7122 - loss: 1.0390 - val_accuracy: 0.0592 - val_loss: 27.6889\n",
            "Epoch 222/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7145 - loss: 1.0427\n",
            "Epoch 222: accuracy improved from 0.70924 to 0.71030, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.7144 - loss: 1.0428 - val_accuracy: 0.0597 - val_loss: 27.0296\n",
            "Epoch 223/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7256 - loss: 1.0009\n",
            "Epoch 223: accuracy improved from 0.71030 to 0.71224, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7254 - loss: 1.0012 - val_accuracy: 0.0591 - val_loss: 27.5721\n",
            "Epoch 224/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7192 - loss: 1.0188\n",
            "Epoch 224: accuracy did not improve from 0.71224\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.7192 - loss: 1.0189 - val_accuracy: 0.0597 - val_loss: 27.5958\n",
            "Epoch 225/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7178 - loss: 1.0265\n",
            "Epoch 225: accuracy did not improve from 0.71224\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7177 - loss: 1.0266 - val_accuracy: 0.0613 - val_loss: 27.2848\n",
            "Epoch 226/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7128 - loss: 1.0289\n",
            "Epoch 226: accuracy improved from 0.71224 to 0.71252, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.7128 - loss: 1.0290 - val_accuracy: 0.0600 - val_loss: 27.6317\n",
            "Epoch 227/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7212 - loss: 1.0022\n",
            "Epoch 227: accuracy improved from 0.71252 to 0.71470, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.7212 - loss: 1.0023 - val_accuracy: 0.0615 - val_loss: 27.3638\n",
            "Epoch 228/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7236 - loss: 0.9889\n",
            "Epoch 228: accuracy improved from 0.71470 to 0.71692, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.7235 - loss: 0.9892 - val_accuracy: 0.0592 - val_loss: 27.7272\n",
            "Epoch 229/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7223 - loss: 1.0103\n",
            "Epoch 229: accuracy did not improve from 0.71692\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7222 - loss: 1.0106 - val_accuracy: 0.0600 - val_loss: 27.9446\n",
            "Epoch 230/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7311 - loss: 0.9778\n",
            "Epoch 230: accuracy improved from 0.71692 to 0.72344, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.7310 - loss: 0.9781 - val_accuracy: 0.0605 - val_loss: 27.9170\n",
            "Epoch 231/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7281 - loss: 0.9811\n",
            "Epoch 231: accuracy did not improve from 0.72344\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7281 - loss: 0.9813 - val_accuracy: 0.0584 - val_loss: 28.1120\n",
            "Epoch 232/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7213 - loss: 1.0062\n",
            "Epoch 232: accuracy did not improve from 0.72344\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7212 - loss: 1.0063 - val_accuracy: 0.0588 - val_loss: 27.9305\n",
            "Epoch 233/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7247 - loss: 0.9949\n",
            "Epoch 233: accuracy did not improve from 0.72344\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7247 - loss: 0.9951 - val_accuracy: 0.0584 - val_loss: 27.9181\n",
            "Epoch 234/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7194 - loss: 1.0083\n",
            "Epoch 234: accuracy did not improve from 0.72344\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7194 - loss: 1.0084 - val_accuracy: 0.0614 - val_loss: 27.9177\n",
            "Epoch 235/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7229 - loss: 0.9985\n",
            "Epoch 235: accuracy did not improve from 0.72344\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7229 - loss: 0.9985 - val_accuracy: 0.0596 - val_loss: 27.8842\n",
            "Epoch 236/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7285 - loss: 0.9687\n",
            "Epoch 236: accuracy did not improve from 0.72344\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7285 - loss: 0.9690 - val_accuracy: 0.0588 - val_loss: 28.0226\n",
            "Epoch 237/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7352 - loss: 0.9604\n",
            "Epoch 237: accuracy improved from 0.72344 to 0.72538, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7352 - loss: 0.9606 - val_accuracy: 0.0581 - val_loss: 28.1543\n",
            "Epoch 238/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7360 - loss: 0.9636\n",
            "Epoch 238: accuracy improved from 0.72538 to 0.72803, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7360 - loss: 0.9637 - val_accuracy: 0.0598 - val_loss: 27.6970\n",
            "Epoch 239/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7366 - loss: 0.9381\n",
            "Epoch 239: accuracy improved from 0.72803 to 0.72909, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7365 - loss: 0.9384 - val_accuracy: 0.0602 - val_loss: 28.2650\n",
            "Epoch 240/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7384 - loss: 0.9360\n",
            "Epoch 240: accuracy improved from 0.72909 to 0.73006, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.7383 - loss: 0.9363 - val_accuracy: 0.0602 - val_loss: 28.8560\n",
            "Epoch 241/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7357 - loss: 0.9529\n",
            "Epoch 241: accuracy did not improve from 0.73006\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.7356 - loss: 0.9530 - val_accuracy: 0.0589 - val_loss: 28.2704\n",
            "Epoch 242/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7344 - loss: 0.9722\n",
            "Epoch 242: accuracy improved from 0.73006 to 0.73144, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.7343 - loss: 0.9723 - val_accuracy: 0.0578 - val_loss: 27.9121\n",
            "Epoch 243/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7374 - loss: 0.9431\n",
            "Epoch 243: accuracy did not improve from 0.73144\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7373 - loss: 0.9433 - val_accuracy: 0.0578 - val_loss: 27.9960\n",
            "Epoch 244/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7402 - loss: 0.9239\n",
            "Epoch 244: accuracy improved from 0.73144 to 0.73549, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7402 - loss: 0.9240 - val_accuracy: 0.0607 - val_loss: 27.6868\n",
            "Epoch 245/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7366 - loss: 0.9293\n",
            "Epoch 245: accuracy did not improve from 0.73549\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7366 - loss: 0.9294 - val_accuracy: 0.0600 - val_loss: 28.2635\n",
            "Epoch 246/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7382 - loss: 0.9305\n",
            "Epoch 246: accuracy did not improve from 0.73549\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.7382 - loss: 0.9306 - val_accuracy: 0.0596 - val_loss: 28.6032\n",
            "Epoch 247/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7412 - loss: 0.9409\n",
            "Epoch 247: accuracy did not improve from 0.73549\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7411 - loss: 0.9411 - val_accuracy: 0.0577 - val_loss: 28.0249\n",
            "Epoch 248/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7478 - loss: 0.9108\n",
            "Epoch 248: accuracy improved from 0.73549 to 0.73609, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.7477 - loss: 0.9111 - val_accuracy: 0.0592 - val_loss: 28.2861\n",
            "Epoch 249/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7394 - loss: 0.9292\n",
            "Epoch 249: accuracy improved from 0.73609 to 0.73655, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.7394 - loss: 0.9293 - val_accuracy: 0.0593 - val_loss: 28.7182\n",
            "Epoch 250/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7483 - loss: 0.9044\n",
            "Epoch 250: accuracy improved from 0.73655 to 0.73852, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7483 - loss: 0.9045 - val_accuracy: 0.0586 - val_loss: 28.2523\n",
            "Epoch 251/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7366 - loss: 0.9402\n",
            "Epoch 251: accuracy did not improve from 0.73852\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.7366 - loss: 0.9402 - val_accuracy: 0.0588 - val_loss: 28.3154\n",
            "Epoch 252/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7392 - loss: 0.9310\n",
            "Epoch 252: accuracy improved from 0.73852 to 0.73921, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7392 - loss: 0.9311 - val_accuracy: 0.0584 - val_loss: 27.8127\n",
            "Epoch 253/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7481 - loss: 0.9104\n",
            "Epoch 253: accuracy improved from 0.73921 to 0.74342, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7481 - loss: 0.9107 - val_accuracy: 0.0579 - val_loss: 27.9530\n",
            "Epoch 254/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7524 - loss: 0.8937\n",
            "Epoch 254: accuracy improved from 0.74342 to 0.74673, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7523 - loss: 0.8938 - val_accuracy: 0.0614 - val_loss: 28.3787\n",
            "Epoch 255/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7450 - loss: 0.9083\n",
            "Epoch 255: accuracy did not improve from 0.74673\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7449 - loss: 0.9085 - val_accuracy: 0.0593 - val_loss: 28.4899\n",
            "Epoch 256/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7545 - loss: 0.8822\n",
            "Epoch 256: accuracy improved from 0.74673 to 0.74798, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7544 - loss: 0.8824 - val_accuracy: 0.0610 - val_loss: 28.8433\n",
            "Epoch 257/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7450 - loss: 0.9099\n",
            "Epoch 257: accuracy did not improve from 0.74798\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7450 - loss: 0.9100 - val_accuracy: 0.0581 - val_loss: 28.7022\n",
            "Epoch 258/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7515 - loss: 0.8921\n",
            "Epoch 258: accuracy did not improve from 0.74798\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.7514 - loss: 0.8924 - val_accuracy: 0.0589 - val_loss: 28.5082\n",
            "Epoch 259/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7494 - loss: 0.8933\n",
            "Epoch 259: accuracy did not improve from 0.74798\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - accuracy: 0.7494 - loss: 0.8934 - val_accuracy: 0.0574 - val_loss: 28.4319\n",
            "Epoch 260/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7507 - loss: 0.8952\n",
            "Epoch 260: accuracy did not improve from 0.74798\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.7507 - loss: 0.8953 - val_accuracy: 0.0607 - val_loss: 28.3616\n",
            "Epoch 261/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7547 - loss: 0.8705\n",
            "Epoch 261: accuracy improved from 0.74798 to 0.75188, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7546 - loss: 0.8707 - val_accuracy: 0.0589 - val_loss: 28.2011\n",
            "Epoch 262/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7536 - loss: 0.8961\n",
            "Epoch 262: accuracy did not improve from 0.75188\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7535 - loss: 0.8962 - val_accuracy: 0.0558 - val_loss: 28.1042\n",
            "Epoch 263/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7536 - loss: 0.8684\n",
            "Epoch 263: accuracy did not improve from 0.75188\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.7535 - loss: 0.8688 - val_accuracy: 0.0602 - val_loss: 28.6436\n",
            "Epoch 264/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7528 - loss: 0.8820\n",
            "Epoch 264: accuracy did not improve from 0.75188\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7528 - loss: 0.8821 - val_accuracy: 0.0588 - val_loss: 28.5241\n",
            "Epoch 265/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7514 - loss: 0.8933\n",
            "Epoch 265: accuracy did not improve from 0.75188\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7513 - loss: 0.8933 - val_accuracy: 0.0582 - val_loss: 28.2780\n",
            "Epoch 266/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7609 - loss: 0.8684\n",
            "Epoch 266: accuracy improved from 0.75188 to 0.75463, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7609 - loss: 0.8685 - val_accuracy: 0.0592 - val_loss: 28.4408\n",
            "Epoch 267/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7557 - loss: 0.8668\n",
            "Epoch 267: accuracy did not improve from 0.75463\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7556 - loss: 0.8669 - val_accuracy: 0.0628 - val_loss: 28.8014\n",
            "Epoch 268/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7615 - loss: 0.8475\n",
            "Epoch 268: accuracy did not improve from 0.75463\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.7614 - loss: 0.8478 - val_accuracy: 0.0593 - val_loss: 28.6455\n",
            "Epoch 269/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7627 - loss: 0.8502\n",
            "Epoch 269: accuracy improved from 0.75463 to 0.75569, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7626 - loss: 0.8503 - val_accuracy: 0.0605 - val_loss: 28.5657\n",
            "Epoch 270/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7616 - loss: 0.8489\n",
            "Epoch 270: accuracy improved from 0.75569 to 0.75634, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7616 - loss: 0.8491 - val_accuracy: 0.0622 - val_loss: 28.9277\n",
            "Epoch 271/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7612 - loss: 0.8571\n",
            "Epoch 271: accuracy improved from 0.75634 to 0.75744, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.7612 - loss: 0.8572 - val_accuracy: 0.0598 - val_loss: 28.7761\n",
            "Epoch 272/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7544 - loss: 0.8713\n",
            "Epoch 272: accuracy did not improve from 0.75744\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7544 - loss: 0.8712 - val_accuracy: 0.0587 - val_loss: 29.1153\n",
            "Epoch 273/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7655 - loss: 0.8445\n",
            "Epoch 273: accuracy improved from 0.75744 to 0.76068, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7654 - loss: 0.8446 - val_accuracy: 0.0579 - val_loss: 28.8046\n",
            "Epoch 274/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7592 - loss: 0.8544\n",
            "Epoch 274: accuracy did not improve from 0.76068\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7592 - loss: 0.8545 - val_accuracy: 0.0598 - val_loss: 28.6184\n",
            "Epoch 275/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7665 - loss: 0.8419\n",
            "Epoch 275: accuracy improved from 0.76068 to 0.76499, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.7665 - loss: 0.8419 - val_accuracy: 0.0600 - val_loss: 28.7808\n",
            "Epoch 276/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7621 - loss: 0.8374\n",
            "Epoch 276: accuracy did not improve from 0.76499\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7622 - loss: 0.8375 - val_accuracy: 0.0591 - val_loss: 29.0814\n",
            "Epoch 277/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7666 - loss: 0.8387\n",
            "Epoch 277: accuracy did not improve from 0.76499\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7665 - loss: 0.8389 - val_accuracy: 0.0608 - val_loss: 28.9129\n",
            "Epoch 278/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7627 - loss: 0.8500\n",
            "Epoch 278: accuracy did not improve from 0.76499\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7626 - loss: 0.8500 - val_accuracy: 0.0591 - val_loss: 28.5227\n",
            "Epoch 279/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7656 - loss: 0.8322\n",
            "Epoch 279: accuracy did not improve from 0.76499\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7655 - loss: 0.8324 - val_accuracy: 0.0589 - val_loss: 29.1891\n",
            "Epoch 280/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7722 - loss: 0.8233\n",
            "Epoch 280: accuracy improved from 0.76499 to 0.76668, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.7722 - loss: 0.8235 - val_accuracy: 0.0587 - val_loss: 28.8870\n",
            "Epoch 281/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7679 - loss: 0.8421\n",
            "Epoch 281: accuracy did not improve from 0.76668\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.7678 - loss: 0.8423 - val_accuracy: 0.0579 - val_loss: 28.7307\n",
            "Epoch 282/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7682 - loss: 0.8320\n",
            "Epoch 282: accuracy did not improve from 0.76668\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7681 - loss: 0.8320 - val_accuracy: 0.0579 - val_loss: 29.0022\n",
            "Epoch 283/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7713 - loss: 0.8159\n",
            "Epoch 283: accuracy did not improve from 0.76668\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.7712 - loss: 0.8162 - val_accuracy: 0.0599 - val_loss: 29.3605\n",
            "Epoch 284/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7713 - loss: 0.8210\n",
            "Epoch 284: accuracy did not improve from 0.76668\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - accuracy: 0.7712 - loss: 0.8211 - val_accuracy: 0.0607 - val_loss: 28.6636\n",
            "Epoch 285/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7727 - loss: 0.8149\n",
            "Epoch 285: accuracy improved from 0.76668 to 0.76874, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.7726 - loss: 0.8150 - val_accuracy: 0.0587 - val_loss: 28.8522\n",
            "Epoch 286/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7711 - loss: 0.8007\n",
            "Epoch 286: accuracy did not improve from 0.76874\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7711 - loss: 0.8008 - val_accuracy: 0.0609 - val_loss: 29.1223\n",
            "Epoch 287/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7788 - loss: 0.7839\n",
            "Epoch 287: accuracy improved from 0.76874 to 0.77052, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.7788 - loss: 0.7840 - val_accuracy: 0.0594 - val_loss: 28.7879\n",
            "Epoch 288/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7678 - loss: 0.8329\n",
            "Epoch 288: accuracy did not improve from 0.77052\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7678 - loss: 0.8331 - val_accuracy: 0.0614 - val_loss: 28.7861\n",
            "Epoch 289/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7721 - loss: 0.8274\n",
            "Epoch 289: accuracy did not improve from 0.77052\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - accuracy: 0.7721 - loss: 0.8275 - val_accuracy: 0.0603 - val_loss: 28.4920\n",
            "Epoch 290/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7726 - loss: 0.8016\n",
            "Epoch 290: accuracy did not improve from 0.77052\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7725 - loss: 0.8019 - val_accuracy: 0.0592 - val_loss: 28.7757\n",
            "Epoch 291/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7749 - loss: 0.8008\n",
            "Epoch 291: accuracy improved from 0.77052 to 0.77142, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.7749 - loss: 0.8010 - val_accuracy: 0.0607 - val_loss: 29.1553\n",
            "Epoch 292/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7812 - loss: 0.7781\n",
            "Epoch 292: accuracy improved from 0.77142 to 0.77448, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 28ms/step - accuracy: 0.7811 - loss: 0.7784 - val_accuracy: 0.0627 - val_loss: 29.1008\n",
            "Epoch 293/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7748 - loss: 0.7926\n",
            "Epoch 293: accuracy did not improve from 0.77448\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7747 - loss: 0.7929 - val_accuracy: 0.0593 - val_loss: 28.9810\n",
            "Epoch 294/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7737 - loss: 0.8005\n",
            "Epoch 294: accuracy did not improve from 0.77448\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7737 - loss: 0.8006 - val_accuracy: 0.0576 - val_loss: 29.3269\n",
            "Epoch 295/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7824 - loss: 0.7722\n",
            "Epoch 295: accuracy improved from 0.77448 to 0.77520, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.7823 - loss: 0.7725 - val_accuracy: 0.0588 - val_loss: 28.9196\n",
            "Epoch 296/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7771 - loss: 0.7915\n",
            "Epoch 296: accuracy improved from 0.77520 to 0.77613, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7771 - loss: 0.7916 - val_accuracy: 0.0574 - val_loss: 29.0860\n",
            "Epoch 297/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7792 - loss: 0.7835\n",
            "Epoch 297: accuracy did not improve from 0.77613\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.7792 - loss: 0.7837 - val_accuracy: 0.0608 - val_loss: 29.2385\n",
            "Epoch 298/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7821 - loss: 0.7707\n",
            "Epoch 298: accuracy improved from 0.77613 to 0.77801, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.7820 - loss: 0.7708 - val_accuracy: 0.0577 - val_loss: 29.2013\n",
            "Epoch 299/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7791 - loss: 0.7730\n",
            "Epoch 299: accuracy did not improve from 0.77801\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7791 - loss: 0.7731 - val_accuracy: 0.0598 - val_loss: 29.5470\n",
            "Epoch 300/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7816 - loss: 0.7715\n",
            "Epoch 300: accuracy did not improve from 0.77801\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7816 - loss: 0.7717 - val_accuracy: 0.0596 - val_loss: 29.3617\n",
            "Epoch 301/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7791 - loss: 0.7951\n",
            "Epoch 301: accuracy did not improve from 0.77801\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7790 - loss: 0.7952 - val_accuracy: 0.0603 - val_loss: 29.1421\n",
            "Epoch 302/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7864 - loss: 0.7551\n",
            "Epoch 302: accuracy improved from 0.77801 to 0.77866, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7863 - loss: 0.7552 - val_accuracy: 0.0587 - val_loss: 29.0517\n",
            "Epoch 303/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7895 - loss: 0.7570\n",
            "Epoch 303: accuracy improved from 0.77866 to 0.78400, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.7895 - loss: 0.7570 - val_accuracy: 0.0600 - val_loss: 29.2992\n",
            "Epoch 304/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7945 - loss: 0.7337\n",
            "Epoch 304: accuracy improved from 0.78400 to 0.79015, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.7945 - loss: 0.7338 - val_accuracy: 0.0594 - val_loss: 29.5180\n",
            "Epoch 305/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7896 - loss: 0.7383\n",
            "Epoch 305: accuracy did not improve from 0.79015\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7895 - loss: 0.7386 - val_accuracy: 0.0587 - val_loss: 29.5203\n",
            "Epoch 306/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7861 - loss: 0.7520\n",
            "Epoch 306: accuracy did not improve from 0.79015\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.7861 - loss: 0.7523 - val_accuracy: 0.0579 - val_loss: 29.8189\n",
            "Epoch 307/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7886 - loss: 0.7488\n",
            "Epoch 307: accuracy did not improve from 0.79015\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7885 - loss: 0.7492 - val_accuracy: 0.0597 - val_loss: 29.3843\n",
            "Epoch 308/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7791 - loss: 0.7651\n",
            "Epoch 308: accuracy did not improve from 0.79015\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7791 - loss: 0.7652 - val_accuracy: 0.0634 - val_loss: 29.1911\n",
            "Epoch 309/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7844 - loss: 0.7852\n",
            "Epoch 309: accuracy did not improve from 0.79015\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7844 - loss: 0.7852 - val_accuracy: 0.0604 - val_loss: 29.0429\n",
            "Epoch 310/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7883 - loss: 0.7612\n",
            "Epoch 310: accuracy did not improve from 0.79015\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7883 - loss: 0.7613 - val_accuracy: 0.0578 - val_loss: 28.9592\n",
            "Epoch 311/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7836 - loss: 0.7545\n",
            "Epoch 311: accuracy did not improve from 0.79015\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7836 - loss: 0.7546 - val_accuracy: 0.0589 - val_loss: 29.4048\n",
            "Epoch 312/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7879 - loss: 0.7482\n",
            "Epoch 312: accuracy did not improve from 0.79015\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7878 - loss: 0.7483 - val_accuracy: 0.0588 - val_loss: 29.5252\n",
            "Epoch 313/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7941 - loss: 0.7480\n",
            "Epoch 313: accuracy did not improve from 0.79015\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7940 - loss: 0.7481 - val_accuracy: 0.0598 - val_loss: 29.4123\n",
            "Epoch 314/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7916 - loss: 0.7364\n",
            "Epoch 314: accuracy did not improve from 0.79015\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 24ms/step - accuracy: 0.7916 - loss: 0.7365 - val_accuracy: 0.0569 - val_loss: 29.3992\n",
            "Epoch 315/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7866 - loss: 0.7539\n",
            "Epoch 315: accuracy did not improve from 0.79015\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.7866 - loss: 0.7540 - val_accuracy: 0.0586 - val_loss: 29.0201\n",
            "Epoch 316/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7921 - loss: 0.7363\n",
            "Epoch 316: accuracy did not improve from 0.79015\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.7921 - loss: 0.7365 - val_accuracy: 0.0593 - val_loss: 29.2896\n",
            "Epoch 317/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7933 - loss: 0.7227\n",
            "Epoch 317: accuracy did not improve from 0.79015\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7933 - loss: 0.7231 - val_accuracy: 0.0586 - val_loss: 29.3958\n",
            "Epoch 318/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7893 - loss: 0.7563\n",
            "Epoch 318: accuracy did not improve from 0.79015\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7892 - loss: 0.7563 - val_accuracy: 0.0574 - val_loss: 29.2221\n",
            "Epoch 319/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7998 - loss: 0.7158\n",
            "Epoch 319: accuracy improved from 0.79015 to 0.79443, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.7997 - loss: 0.7158 - val_accuracy: 0.0605 - val_loss: 29.5704\n",
            "Epoch 320/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7941 - loss: 0.7327\n",
            "Epoch 320: accuracy did not improve from 0.79443\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7941 - loss: 0.7328 - val_accuracy: 0.0613 - val_loss: 29.4940\n",
            "Epoch 321/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8000 - loss: 0.7123\n",
            "Epoch 321: accuracy did not improve from 0.79443\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7999 - loss: 0.7125 - val_accuracy: 0.0620 - val_loss: 29.6073\n",
            "Epoch 322/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7889 - loss: 0.7622\n",
            "Epoch 322: accuracy did not improve from 0.79443\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7889 - loss: 0.7622 - val_accuracy: 0.0602 - val_loss: 29.3651\n",
            "Epoch 323/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7925 - loss: 0.7334\n",
            "Epoch 323: accuracy did not improve from 0.79443\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7925 - loss: 0.7334 - val_accuracy: 0.0632 - val_loss: 29.7817\n",
            "Epoch 324/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7976 - loss: 0.7197\n",
            "Epoch 324: accuracy did not improve from 0.79443\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7976 - loss: 0.7198 - val_accuracy: 0.0599 - val_loss: 29.3218\n",
            "Epoch 325/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7938 - loss: 0.7232\n",
            "Epoch 325: accuracy did not improve from 0.79443\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.7937 - loss: 0.7234 - val_accuracy: 0.0599 - val_loss: 29.5958\n",
            "Epoch 326/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7933 - loss: 0.7314\n",
            "Epoch 326: accuracy did not improve from 0.79443\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7933 - loss: 0.7315 - val_accuracy: 0.0591 - val_loss: 29.9473\n",
            "Epoch 327/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7984 - loss: 0.7145\n",
            "Epoch 327: accuracy improved from 0.79443 to 0.79458, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7984 - loss: 0.7146 - val_accuracy: 0.0574 - val_loss: 29.0100\n",
            "Epoch 328/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7975 - loss: 0.7283\n",
            "Epoch 328: accuracy did not improve from 0.79458\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7975 - loss: 0.7284 - val_accuracy: 0.0591 - val_loss: 29.0550\n",
            "Epoch 329/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8009 - loss: 0.7089\n",
            "Epoch 329: accuracy did not improve from 0.79458\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.8009 - loss: 0.7092 - val_accuracy: 0.0586 - val_loss: 29.5382\n",
            "Epoch 330/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8001 - loss: 0.6968\n",
            "Epoch 330: accuracy did not improve from 0.79458\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.8001 - loss: 0.6969 - val_accuracy: 0.0599 - val_loss: 29.1474\n",
            "Epoch 331/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7962 - loss: 0.7181\n",
            "Epoch 331: accuracy did not improve from 0.79458\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7961 - loss: 0.7182 - val_accuracy: 0.0597 - val_loss: 29.3134\n",
            "Epoch 332/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.7983 - loss: 0.7033\n",
            "Epoch 332: accuracy improved from 0.79458 to 0.79480, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.7982 - loss: 0.7036 - val_accuracy: 0.0569 - val_loss: 29.2563\n",
            "Epoch 333/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8025 - loss: 0.6986\n",
            "Epoch 333: accuracy improved from 0.79480 to 0.79823, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.8024 - loss: 0.6988 - val_accuracy: 0.0609 - val_loss: 29.5083\n",
            "Epoch 334/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8041 - loss: 0.6878\n",
            "Epoch 334: accuracy improved from 0.79823 to 0.79836, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.8041 - loss: 0.6879 - val_accuracy: 0.0589 - val_loss: 29.6249\n",
            "Epoch 335/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.7991 - loss: 0.7129\n",
            "Epoch 335: accuracy did not improve from 0.79836\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.7991 - loss: 0.7130 - val_accuracy: 0.0584 - val_loss: 29.0458\n",
            "Epoch 336/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8045 - loss: 0.6966\n",
            "Epoch 336: accuracy improved from 0.79836 to 0.79889, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.8045 - loss: 0.6967 - val_accuracy: 0.0577 - val_loss: 29.4414\n",
            "Epoch 337/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8040 - loss: 0.6930\n",
            "Epoch 337: accuracy did not improve from 0.79889\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.8039 - loss: 0.6931 - val_accuracy: 0.0587 - val_loss: 29.7330\n",
            "Epoch 338/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8068 - loss: 0.6840\n",
            "Epoch 338: accuracy improved from 0.79889 to 0.80257, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.8068 - loss: 0.6841 - val_accuracy: 0.0582 - val_loss: 29.6259\n",
            "Epoch 339/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8088 - loss: 0.6830\n",
            "Epoch 339: accuracy improved from 0.80257 to 0.80366, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.8088 - loss: 0.6831 - val_accuracy: 0.0594 - val_loss: 29.8448\n",
            "Epoch 340/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8013 - loss: 0.7054\n",
            "Epoch 340: accuracy did not improve from 0.80366\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.8012 - loss: 0.7055 - val_accuracy: 0.0625 - val_loss: 29.6209\n",
            "Epoch 341/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8065 - loss: 0.6946\n",
            "Epoch 341: accuracy did not improve from 0.80366\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.8065 - loss: 0.6947 - val_accuracy: 0.0597 - val_loss: 29.4599\n",
            "Epoch 342/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8063 - loss: 0.6877\n",
            "Epoch 342: accuracy improved from 0.80366 to 0.80395, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 27ms/step - accuracy: 0.8063 - loss: 0.6877 - val_accuracy: 0.0612 - val_loss: 29.8023\n",
            "Epoch 343/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8030 - loss: 0.6994\n",
            "Epoch 343: accuracy did not improve from 0.80395\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.8029 - loss: 0.6994 - val_accuracy: 0.0610 - val_loss: 29.8467\n",
            "Epoch 344/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8098 - loss: 0.6693\n",
            "Epoch 344: accuracy improved from 0.80395 to 0.80860, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.8098 - loss: 0.6693 - val_accuracy: 0.0609 - val_loss: 30.0728\n",
            "Epoch 345/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8032 - loss: 0.6888\n",
            "Epoch 345: accuracy did not improve from 0.80860\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - accuracy: 0.8031 - loss: 0.6890 - val_accuracy: 0.0579 - val_loss: 29.3806\n",
            "Epoch 346/350\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8122 - loss: 0.6690\n",
            "Epoch 346: accuracy did not improve from 0.80860\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.8122 - loss: 0.6690 - val_accuracy: 0.0572 - val_loss: 30.1659\n",
            "Epoch 347/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8050 - loss: 0.6940\n",
            "Epoch 347: accuracy did not improve from 0.80860\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.8050 - loss: 0.6942 - val_accuracy: 0.0591 - val_loss: 29.5936\n",
            "Epoch 348/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8070 - loss: 0.6845\n",
            "Epoch 348: accuracy did not improve from 0.80860\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.8069 - loss: 0.6847 - val_accuracy: 0.0607 - val_loss: 29.7833\n",
            "Epoch 349/350\n",
            "\u001b[1m249/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.8127 - loss: 0.6573\n",
            "Epoch 349: accuracy improved from 0.80860 to 0.80872, saving model to nextword_best_model.keras\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.8126 - loss: 0.6575 - val_accuracy: 0.0582 - val_loss: 30.1083\n",
            "Epoch 350/350\n",
            "\u001b[1m250/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8074 - loss: 0.6824\n",
            "Epoch 350: accuracy did not improve from 0.80872\n",
            "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - accuracy: 0.8074 - loss: 0.6825 - val_accuracy: 0.0579 - val_loss: 29.7765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"nextword_model_v1.h5\")\n",
        "\n",
        "import pickle\n",
        "with open(\"tokenizer_v1.pkl\", \"wb\") as f:\n",
        "    pickle.dump(tokenizer, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-Cs6vHcQfHz",
        "outputId": "ebe0e992-1c53-49f6-b45f-3c734b029933"
      },
      "id": "S-Cs6vHcQfHz",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "model = load_model(\"nextword_model_v1.h5\")\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=2,batch_size=256, validation_data=(X_val, y_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5HmlodOZonN",
        "outputId": "d6d461eb-f8aa-4ac4-f3bf-38cfadae4d3c"
      },
      "id": "N5HmlodOZonN",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 248ms/step - accuracy: 0.8209 - loss: 0.6376 - val_accuracy: 0.0607 - val_loss: 30.1849\n",
            "Epoch 2/2\n",
            "\u001b[1m126/126\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 255ms/step - accuracy: 0.8270 - loss: 0.6056 - val_accuracy: 0.0603 - val_loss: 30.8366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "model = load_model(\"nextword_model_v1.h5\")\n",
        "with open(\"tokenizer_v1.pkl\", \"rb\") as f:\n",
        "    tokenizer = pickle.load(f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SgdC4suPiRt",
        "outputId": "39539d45-3027-4ca8-9328-acaeaddb114b"
      },
      "id": "9SgdC4suPiRt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "def sample(preds, temperature=0.6):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds + 1e-8) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    return np.random.choice(len(preds), p=preds)\n",
        "def predict_next_word_safe(seed_text, tokenizer, max_sequence_len, model, temperature=0.8):\n",
        "    banned_words = {\n",
        "        \"19teens\", \"fuck\", \"shit\", \"bitch\", \"www\", \"com\", \"http\",\n",
        "        \"te\", \"o\", \"k\", \"e\", \"n\", \"start\", \"shadowbots\"\n",
        "    }\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
        "    predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "    while True:\n",
        "        predicted_index = sample(predicted_probs, temperature)\n",
        "        if predicted_index == 0:\n",
        "            continue\n",
        "        word = tokenizer.index_word.get(predicted_index, None)\n",
        "        if word and word not in banned_words and len(word) > 1:\n",
        "            return word\n",
        "test_sentences = [\n",
        "    \"I really like\",\n",
        "    \"He is not\",\n",
        "    \"They are going\",\n",
        "    \"We have to\",\n",
        "    \"I was just\",\n",
        "    \"You can try\",\n",
        "    \"It feels like\",\n",
        "    \"Don’t forget to\",\n",
        "    \"Can you make\",\n",
        "    \"How can I\",\n",
        "    \"Why don’t you\",\n",
        "    \"I should probably\",\n",
        "    \"Everything is so\",\n",
        "    \"Do you think\",\n",
        "    \"I hope you\",\n",
        "    \"I am trying to\",\n",
        "    \"You must be\",\n",
        "    \"We should go\",\n",
        "    \"Please make sure to\",\n",
        "    \"It is very\",\n",
        "    \"I think you\",\n",
        "    \"You will need to\"\n",
        "]\n",
        "for seed in test_sentences:\n",
        "    next_word = predict_next_word_safe(seed, tokenizer, max_sequence_len, model, temperature=0.8)\n",
        "    print(f\"Seed: {seed} → Next word: {next_word}\")\n"
      ],
      "metadata": {
        "id": "UfR--IOss70m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c7b64f4-0c85-4986-de87-b1027adb591e"
      },
      "id": "UfR--IOss70m",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed: I really like → Next word: item\n",
            "Seed: He is not → Next word: blocked\n",
            "Seed: They are going → Next word: backward\n",
            "Seed: We have to → Next word: enter\n",
            "Seed: I was just → Next word: shouldn\n",
            "Seed: You can try → Next word: for\n",
            "Seed: It feels like → Next word: blank\n",
            "Seed: Don’t forget to → Next word: load\n",
            "Seed: Can you make → Next word: keyboard\n",
            "Seed: How can I → Next word: load\n",
            "Seed: Why don’t you → Next word: disappear\n",
            "Seed: I should probably → Next word: specific\n",
            "Seed: Everything is so → Next word: listed\n",
            "Seed: Do you think → Next word: open\n",
            "Seed: I hope you → Next word: click\n",
            "Seed: I am trying to → Next word: detect\n",
            "Seed: You must be → Next word: default\n",
            "Seed: We should go → Next word: back\n",
            "Seed: Please make sure to → Next word: licensing\n",
            "Seed: It is very → Next word: slow\n",
            "Seed: I think you → Next word: render\n",
            "Seed: You will need to → Next word: load\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "model = load_model(\"nextword_model_v1.h5\")\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=1, validation_data=(X_val, y_val))\n"
      ],
      "metadata": {
        "id": "pMD-G3o2QcER"
      },
      "id": "pMD-G3o2QcER",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}